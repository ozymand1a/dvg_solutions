{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Build Your Own NN Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this homework is simple, yet an actual implementation may take some time. We are going to write an Artificial Neural Network (almost) from scratch. The software design was heavily inspired by [PyTorch](http://pytorch.org) which is the main framework of our course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your homework task comprises 4 steps:\n",
    " - Implement all modules in `homework_part1_modules.ipynb` . These modules will be checked automatically using tests provided in `homework_part1_test_modules.ipynb`\n",
    " - Check that your modules work on 'toy example' (see below)\n",
    " - Train small neural network to classify images from mnist dataset\n",
    " - Train the same neural network in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement everything in `homework_part1_modules.ipynb`. Read all the comments thoughtfully, it will reduce the amount of time spended on debug. Please don't change the prototypes since layers you implement will be automatically checked.\n",
    "\n",
    "The typical assumption is that `module.backward` is always executed after `module.forward`,\n",
    "so layer output is stored in `_output`. This would be useful for `SoftMax`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech note about inplace operations\n",
    "You can use `np.multiply`, `np.add`, `np.divide`, `np.subtract` with parameter `out` filled instead of `*`,`+`,`/`,`-` for better memory handling through inplace operations\n",
    "\n",
    "Example: suppose you allocated a variable \n",
    "\n",
    "```\n",
    "a = np.zeros(...)\n",
    "```\n",
    "So, instead of\n",
    "```\n",
    "a = b + c  # will be reallocated, GC needed to free\n",
    "``` \n",
    "You can use: \n",
    "```\n",
    "np.add(b, c, out=a) # puts result in `a`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (re-)load layers\n",
    "%run homework_part1_modules.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have implemented the modules, you can check that your framework can actually be used for neural network training. Use this example to debug your code, start with logistic regression and then test other layers. You do not need to change anything here. This code is provided for you to test the layers. Also it is easy to use this code in MNIST task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "n_samples = 500\n",
    "\n",
    "X_pos = np.random.randn(n_samples, 2) + np.array([2, 2])\n",
    "X_neg = np.random.randn(n_samples, 2) + np.array([-2, -2])\n",
    "X = np.vstack([X_pos, X_neg])\n",
    "\n",
    "Y = np.concatenate([np.ones(n_samples), np.zeros(n_samples)])[:, np.newaxis]\n",
    "Y = np.hstack([Y, 1 - Y])  # one-hot encoding\n",
    "\n",
    "plt.scatter(X[:, 0],X[:, 1], c=Y[:, 0], edgecolors='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a **logistic regression** for debugging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Sequential()\n",
    "net.add_module(Linear(2, 2))\n",
    "net.add_module(LogSoftMax())\n",
    "\n",
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "print(net)\n",
    "\n",
    "# Test something like that then \n",
    "\n",
    "#net = Sequential()\n",
    "#net.add_module(Linear(2, 4))\n",
    "#net.add_module(ReLU())\n",
    "#net.add_module(Linear(4, 2))\n",
    "#net.add_module(LogSoftMax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with simple SGD with momentum. You can test Adam if sgd works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(net, lr=1e-1, momentum=0.9)\n",
    "#opt = Adam(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with batch_size = 1000 to make sure every step lowers the loss, then try stochastic version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch generator\n",
    "def get_batches(dataset, batch_size):\n",
    "    X, Y = dataset\n",
    "    n_samples = X.shape[0]\n",
    "        \n",
    "    # Shuffle at the start of epoch\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_idx = indices[start:end]\n",
    "    \n",
    "        yield X[batch_idx], Y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic training loop. Examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_network(net, optimizer, X, Y, batch_size, n_epoch):\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(n_epoch):\n",
    "        for x_batch, y_batch in get_batches((X, Y), batch_size):\n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            predictions = net.forward(x_batch)\n",
    "            loss = criterion.forward(predictions, y_batch)\n",
    "\n",
    "            # Backward\n",
    "            dp = criterion.backward(predictions, y_batch)\n",
    "            net.backward(x_batch, dp)\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_history.append(loss)\n",
    "\n",
    "        # Visualize\n",
    "        display.clear_output(wait=True)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        plt.title(\"Training loss\")\n",
    "        plt.xlabel(\"#iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.plot(loss_history, 'b')\n",
    "        plt.show()\n",
    "\n",
    "        print('Current loss: %f' % loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network(net, opt, X, Y, batch_size=128, n_epoch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using old good [MNIST](http://yann.lecun.com/exdb/mnist/) as our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = mnist.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5\n",
    "plt.figure(figsize=(10,4))\n",
    "for i in range(n_samples):\n",
    "    plt.subplot(1,n_samples,i+1)\n",
    "    plt.imshow(X_train[i], 'gray')\n",
    "    plt.title('label:{}'.format(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task:\n",
    "1. Preprocess data: One-hot encode the labels and reshape 28x28 images to 1D vectors of size 784\n",
    "2. Define simple network Linear(128) + relu + Linear(10) + LogSoftMax\n",
    "3. Define criterion ClassNLLCriterion\n",
    "4. Define optimizer (sgd or adam)\n",
    "5. Train network\n",
    "6. Test network on test data, print its accuracy. Make sure accuracy is greater than 95% on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape, y_train[:10])\n",
    "\n",
    "# YOUR CODE: reshape + one-hot encoding\n",
    "\n",
    "X_test = ...\n",
    "X_train = ...\n",
    "\n",
    "y_train = ...\n",
    "y_test = ...\n",
    "\n",
    "assert len(X_test.shape) == 2, X_test.shape\n",
    "assert len(X_test[-1]) == 28*28, X_test.shape\n",
    "assert len(y_test.shape) == 2, y_test.shape\n",
    "assert len(y_test[-1]) == 10, y_test.shape\n",
    "\n",
    "assert len(X_train.shape) == 2, X_train.shape\n",
    "assert len(X_train[-1]) == 28*28, X_train.shape\n",
    "assert len(y_train.shape) == 2, y_train.shape\n",
    "assert len(y_train[-1]) == 10, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: network definition Linear(128) + relu + Linear(10) + LogSoftMax\n",
    "net = Sequential()\n",
    "net.add_module(...)\n",
    "...\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: criterion definition\n",
    "criterion = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: optimizer definition\n",
    "# for sgd normally you can start with momentum = 0.9 or 0.99 and lr=1e-2  or 1e-1\n",
    "opt = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: train network, try batch 128 and 5-15 epochs\n",
    "train_network(net, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test function is already written here for you. Check your network accuracy, it should be above 95% on test set to pass the grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(network, x_test, y_test, batch_size=1):\n",
    "    n_correct = 0\n",
    "    loss_hist = []\n",
    "    for x_batch, y_batch in get_batches((x_test, y_test), batch_size):\n",
    "        # Forward\n",
    "        predictions = network.forward(x_batch)\n",
    "        loss = criterion.forward(predictions, y_batch)\n",
    "        loss_hist.append(loss)\n",
    "        \n",
    "        y_pred = np.argmax(predictions, axis=-1)\n",
    "        y_true = np.argmax(y_batch, axis=-1)\n",
    "        n_correct += np.sum(y_pred == y_true)\n",
    "    print('Mean loss: {}'.format(np.mean(loss_hist)))    \n",
    "    accuracy = n_correct * 100. / len(y_test)\n",
    "    print('accuracy: {}'.format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test_network(net, X_test, y_test)\n",
    "assert accuracy > 95, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pytorch on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same network in pytorch\n",
    "\n",
    "0. Preprocess data. This time gt labels should represent class indices and images should be reshaped from 28x28 images to 1D vectors of size 784.\n",
    "1. Define network`Linear(784, 128) -> Relu -> Linear(128, 10) -> LogSoftMax`\n",
    "  - you can find all layers in `torch.nn` module\n",
    "  - the only difference from our framework will be in `Sequential.add_module` method, check its signature in documentation\n",
    "2. Define criterion\n",
    "  - analogue of `ClassNLLCriterion` is `torch.nn.NLLoss`, but it takes class labeles as the second parameter instead of one-hot encoded labels - that's why we need different preprocessing on step 0.\n",
    "3. Define optimizer from torch.optim module\n",
    "  - optimizers take a list of network parameters instead of the whole network as in our framework. You can easily get the parameters by calling `net.parameters()` method\n",
    "4. Rewrite `train_network()` and `test_network()` functions so that they will be able work with pytorch network\n",
    "  - just convert numpy arrays with batch data to `torch.Tensor` using `torch.from_numpy` and feed them to the network\n",
    "  - result tensors can easily converted from pytorch to numpy using chain of two methods: `detach().numpy()`\n",
    "  - \n",
    "5. Train your network\n",
    "6. Test network on test data, print its accuracy. Make sure accuracy is greater than 95% on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = mnist.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: reshape X_train, X_test and cast y_train, y_test to int64\n",
    "print(X_train.shape, y_train.shape, y_train[:10])\n",
    "\n",
    "X_test = ...\n",
    "X_train = ...\n",
    "\n",
    "y_train = ...\n",
    "y_test = ...\n",
    "\n",
    "assert len(X_test.shape) == 2, X_test.shape\n",
    "assert len(X_test[-1]) == 28*28, X_test.shape\n",
    "assert len(y_test.shape) == 1, y_test.shape\n",
    "\n",
    "assert len(X_train.shape) == 2, X_train.shape\n",
    "assert len(X_train[-1]) == 28*28, X_train.shape\n",
    "assert len(y_train.shape) == 1, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE define network Linear(128) + relu + Linear(10) + LogSoftmax \n",
    "net = torch.nn.Sequential()\n",
    "net.add_module('fc1', torch.nn.Linear(28*28, 128))\n",
    "net.add_module(...)\n",
    "...\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: define criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: define optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network_pytorch(net, opt, X, Y, batch_size, n_epoch):\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(n_epoch):\n",
    "        for x_batch, y_batch in get_batches((X, Y), batch_size):\n",
    "            \n",
    "            # YOUR CODE: rewrite train_network() function to be able to work with pytorch network\n",
    "            pass\n",
    "\n",
    "        # Visualize\n",
    "        display.clear_output(wait=True)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        plt.title(\"Training loss\")\n",
    "        plt.xlabel(\"#iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.plot(loss_history, 'b')\n",
    "        plt.show()\n",
    "\n",
    "        print('Current loss: %f' % loss)    \n",
    "        \n",
    "train_network_pytorch(net, opt, X_train, y_train, 128, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network_pytorch(network, x_test, y_test, batch_size=1):\n",
    "    n_correct = 0\n",
    "    loss_hist = []\n",
    "\n",
    "    for x_batch, y_batch in get_batches((x_test, y_test), batch_size):\n",
    "        # YOUR CODE: rewrite test_network() function to be able to work with pytorch network\n",
    "        pass\n",
    "\n",
    "    print('Mean loss: {}'.format(np.mean(loss_hist)))    \n",
    "    accuracy = n_correct * 100. / len(y_test)\n",
    "    print('accuracy: {}'.format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your network accuracy, it should be above 95% on test set to pass the grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test_network_pytorch(net, X_test, y_test)\n",
    "assert accuracy > 95, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
