{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "hsbyb4tyki9nx32utdtjpk",
    "id": "71AQJg3CDMn9"
   },
   "source": [
    "# Homework - Deep learning for image classification\n",
    "\n",
    "\n",
    "Let's train network to classify images from Tiny ImageNet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your homework contains three parts:\n",
    "\n",
    "1. Make yourself familiar with ordinary training script structure and train good old vgg-like network\n",
    "2. Improve quality with resnet-like network\n",
    "3. Improve quality with test-time augmentation\n",
    "\n",
    "But first of all let's take a look on data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "34l2kkk7t84llsmzyxus1",
    "id": "2MaELIpIDMoA"
   },
   "source": [
    "# Tiny ImageNet dataset\n",
    "In this homework we shall focus on the image recognition problem on Tiny Image Net dataset. This dataset contains\n",
    "* 100k images of shape 3x64x64\n",
    "* 200 different classes: snakes, spiders, cats, trucks, grasshopper, gull, etc.\n",
    "\n",
    "In fact, it is a subset of ImageNet dataset with 4x downscaled images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "rjwf5t0s4f8zbnfmyu7q",
    "id": "swKtJaVyDMoU"
   },
   "source": [
    "## Image examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "nbxuu26h8hhcgzzgeh5nh",
    "id": "h5wImXEaDMoV"
   },
   "source": [
    "\n",
    "\n",
    "<tr>\n",
    "    <td> <img src=\"https://github.com/yandexdataschool/Practical_DL/blob/sem3spring2019/week03_convnets/tinyim3.png?raw=1\" alt=\"Drawing\" style=\"width:90%\"/> </td>\n",
    "    <td> <img src=\"https://github.com/yandexdataschool/Practical_DL/blob/sem3spring2019/week03_convnets/tinyim2.png?raw=1\" alt=\"Drawing\" style=\"width:90%\"/> </td>\n",
    "</tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "w71ngep3y8jm2s3xt0qg",
    "id": "Do-qRQp8DMoW"
   },
   "source": [
    "<tr>\n",
    "    <td> <img src=\"https://github.com/yandexdataschool/Practical_DL/blob/sem3spring2019/week03_convnets/tiniim.png?raw=1\" alt=\"Drawing\" style=\"width:90%\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "k1eayz1ur2mqly9zrk5my",
    "id": "sCvh1ICbHNCE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!S:bash\n",
    "# if you are in colab, just add '!' in the start of the following line\n",
    "# !wget --no-check-certificate 'https://raw.githubusercontent.com/yandexdataschool/deep_vision_and_graphics/fall21/homework01/tiny_img.py' -O tiny_img.py\n",
    "# !wget --no-check-certificate 'https://raw.githubusercontent.com/yandexdataschool/deep_vision_and_graphics/fall21/homework01/tiny_img_dataset.py' -O tiny_img_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "5nh892g5zpl9qv5fki8vpk",
    "id": "5rQhiYyRDMoG"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "# from tiny_img import download_tinyImg200\n",
    "# data_path = '.'\n",
    "# download_tinyImg200(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Training script structure and vgg-like network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a neural network for a specific task you should write code for 4 task-specific blocks and for one task-independed block:\n",
    "1. data loader (data provider) - how to load and augment data for nn training\n",
    "2. neural network architecture - what will be trained\n",
    "3. loss function (+ auxilary metrics on train and validation set) - how to check neural network quality\n",
    "4. optiimzer and training schedule - how neural network will be trained\n",
    "5. \"Train loop\" - what exactly to do for each batch, how often to check validation error, how often to save network and so on. This code could be written in general way and reused between different training scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "g2i37mixtk9kkxkki1y8",
    "id": "rS_-00tYDMoB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our main computing device is 'cuda:0'\n"
     ]
    }
   ],
   "source": [
    "#!L\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "\n",
    "def get_computing_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = get_computing_device()\n",
    "print(f\"Our main computing device is '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data loader and data augmentation\n",
    "Normally there are two connected abstractions for data manipulation:\n",
    "- Dataset (`torch.utils.data.Dataset` and its subclasses from `torchvision.datasets`) - some black-box that keeps and preprocesses separate elements of dataset. In particular, single sample augmentations live on this level usually.\n",
    "- DataLoader (`torch.utils.data.DataLoader`) - structure that combines separate elements in batch.\n",
    "\n",
    "Let's deal with training dataset. Here are some simple augmentations that we are going to use in our experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10839/218740774.py:18: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  resample=PIL.Image.BILINEAR\n",
      "/home/german.petrov/Desktop/dvg_solutions/venv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:1292: UserWarning: The parameter 'resample' is deprecated since 0.12 and will be removed 0.14. Please use 'interpolation' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import PIL\n",
    "\n",
    "# train_trainsforms = transforms.Compose(\n",
    "#     [\n",
    "#         A.HorizontalFlip(),\n",
    "#         A.Rotate(5),\n",
    "#         ToTensorV2(),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "train_trainsforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(\n",
    "            degrees=10,\n",
    "            resample=PIL.Image.BILINEAR\n",
    "        ),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=(0.75, 1.25),\n",
    "            contrast=(0.75, 1.25),\n",
    "            saturation=(0.75, 1.25),\n",
    "            hue=(-0.1, 0.1),\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "     # YOUR CODE : examine torchvision.transforms package, find transformation for color jittering\n",
    "     # and add it with proper parameters.\n",
    "#      transforms.SOME_OTHER_AUGMENTATION_FOR_COLOR_JITTER\n",
    "     # you may add any other transforms here\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training dataset we will use custom dataset that will keep all training data in RAM. If your amount of RAM memory is low, you can use `torchvision.datasets.ImageFolder()` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "jrzsbgniodgtg1hif324k9",
    "id": "5vq5Cm0ADMoK"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "import tiny_img_dataset\n",
    "# you may use torchvision.datasets.ImageFolder() with the same parameters for loading train dataset \n",
    "# train_dataset = tiny_img_dataset.TinyImagenetRAM('tiny-imagenet-200/train', transform=train_trainsforms)\n",
    "train_dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/train', transform=train_trainsforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now validation. Take a look at `tiny-imagenet-200/val` folder and compare it with `tiny-imagenet-200/train`. Looks different, right? So we can't use `TinyImagenetRAM` for loading the validation set. Let's write a custom dataset instead but with the same behavior like `TinyImagenetRAM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class TinyImagenetValDataset(Dataset):\n",
    "    def __init__(self, root, transform=transforms.ToTensor()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = root\n",
    "        with open(os.path.join(root, 'val_annotations.txt')) as f:\n",
    "            annotations = []\n",
    "            for line in f:\n",
    "                img_name, class_label = line.split('\\t')[:2]\n",
    "                annotations.append((img_name, class_label))\n",
    "\n",
    "        # 1. define self.classes - list of sorted class labels from annotations\n",
    "        # it should look like self.classes from \"TinyImagenetRAM\"\n",
    "        # YOUR CODE\n",
    "        self.classes = sorted(np.unique(np.array([annot[1] for annot in annotations])))\n",
    "        \n",
    "        assert len(self.classes) == 200, len(self.classes)\n",
    "        assert all(self.classes[i] < self.classes[i+1] for i in range(len(self.classes)-1)), 'classes should be ordered'\n",
    "        assert all(isinstance(elem, type(annotations[0][1])) for elem in self.classes), 'your just need to reuse class_labels'\n",
    "\n",
    "        # 2. self.class_to_idx - dict from class label to class index\n",
    "        self.class_to_idx = {item: index for index, item in enumerate(self.classes)}\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images, self.targets = [], []\n",
    "        for img_name, class_name in tqdm.tqdm(annotations, desc=root):\n",
    "            img_name = os.path.join(root, 'images', img_name)\n",
    "            # 3. load image and store it in self.images (your may want to use tiny_img_dataset.read_rgb_image)\n",
    "            # store the class index in self.targets\n",
    "            # YOUR CODE\n",
    "            image = tiny_img_dataset.read_rgb_image(img_name)\n",
    "            \n",
    "            assert image.shape == (64, 64, 3), image.shape\n",
    "            self.images.append(Image.fromarray(image))\n",
    "            self.targets.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # take image and its target label from \"self.images\" and \"self.targets\", \n",
    "        # transform the image using self.transform and return the transformed image and its target label\n",
    "        \n",
    "        # YOUR CODE\n",
    "        image = self.images[index]\n",
    "        image = self.transform(image)\n",
    "        target = self.targets[index]\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally load validation dataset. Normally you don't need to augment validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tiny-imagenet-200/val: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:03<00:00, 2596.69it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = TinyImagenetValDataset('tiny-imagenet-200/val', transform=transforms.ToTensor())\n",
    "\n",
    "assert all(train_dataset.classes[i] == val_dataset.classes[i] for i in range(200)), \\\n",
    "    'class order in train and val datasets should be the same'\n",
    "assert all(train_dataset.class_to_idx[elem] == val_dataset.class_to_idx[elem] for elem in train_dataset.classes), \\\n",
    "    'class indices should be the same'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most cases the default `DataLoader` will be good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "6md8io0fesfby4r9per3jb",
    "id": "tY6OUeOODMoN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/german.petrov/Desktop/dvg_solutions/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "#!L\n",
    "batch_size = 64\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_dataset, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "hsq566ut87vokpkiq68",
    "id": "HBgW-gzwDMoQ"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "val_batch_gen = torch.utils.data.DataLoader(val_dataset, \n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "fxzxgbl11g2dixss4t9nx",
    "id": "arxSyhBLDMoX"
   },
   "source": [
    "### 1.2 Neural network definition\n",
    "\n",
    "\"VGG-like network\" usually means that the network is a sequence of convolutions with MaxPooling for downsampling. Here is a table from the original paper [\"Very Deep Convolutional Networks for Large-Scale Image Recognition\"](https://arxiv.org/abs/1409.1556) that describes classical configurations of VGG networks (often referred as VGG-A, VGG-B and so on using column name as an identificator or as VGG16, VGG19 and so on using amount of layers as an identificator)\n",
    "![image.png](https://pytorch.org/assets/images/vgg.png)\n",
    "\n",
    "These network configurations were designed for ImageNet dataset. Since images in tiny-imagenet are 4x downsampled, we are going to design our own configuration by reducing: 1) amount of layers; 2) amount of neurons in layers; 3) amount of maxpooling layers which downsample feature maps\n",
    "\n",
    "Our network config will be [Conv(16), Conv(16), MaxPool] + [Conv(32), Conv(32), MaxPool] + [Conv(64), Conv(64), MaxPool] + [Conv(128), Conv(128)] + [GlobalAveragePooling] + [FC(200) + softmax]\n",
    "\n",
    "We use Conv(128) and GlobalAveragePooling instead of image flattening and FC layers for reducing the amount of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "g5yf9z66xdpvq688ze2d8",
    "id": "7QF2hMVxDMoY"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "6yn15hpuolcmryork2oqs",
    "id": "DJ6QKG3hDMoa"
   },
   "source": [
    "And one more thing. VGG was designed before BatchNormalization was introduced. Nowadays it will be stupid if we don't use batch normalization in our network. So let's define simple module containing convolution, batch norm and relu in it and build our network using this module. Here is also implementation of GlobalAveragePooling given for you as example of custom module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellId": "f985tf2dvssqwmyc6w99d",
    "id": "u_mbfRXMDMob"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "class GlobalAveragePool(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, dim=self.dim)\n",
    "\n",
    "    \n",
    "class ConvBNRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding='same'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # YOUR CODE: define vars for convolution, batchnorm, relu\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE: sequentially apply convolution, batchnorm, relu to 'x'\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def create_vgg_like_network(config=None):\n",
    "    \"\"\"\n",
    "    Creates VGG like network according to config\n",
    "    \"\"\"\n",
    "    model = nn.Sequential()\n",
    "    \n",
    "    default_config = [[16,16], [32, 32], [64, 64], [128, 128]]\n",
    "    config = config or default_config\n",
    "    \n",
    "    in_channels = 3\n",
    "    for block_index in range(len(config)):\n",
    "        for layer_index_in_block in range(len(config[block_index])):\n",
    "            out_channels = config[block_index][layer_index_in_block]\n",
    "            \n",
    "            # YOUR CODE: add ConvBNRelu module to model\n",
    "            conv_bn_relu = ConvBNRelu(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3\n",
    "            )\n",
    "            model.add_module(f'conv_2d_bn_relu_{block_index}_{layer_index_in_block}', conv_bn_relu)\n",
    "            \n",
    "            in_channels = out_channels\n",
    "            \n",
    "        if block_index != len(config) - 1:\n",
    "            model.add_module(f'mp_{block_index}', nn.MaxPool2d(3, stride=2))\n",
    "            \n",
    "    model.add_module('pool', GlobalAveragePool(dim=(2,3)))\n",
    "    model.add_module('logits', nn.Linear(out_channels, 200))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are our model created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_vgg_like_network()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7dh3d8xmkeinv4kx0g079",
    "id": "DvugZZbeDMoe"
   },
   "source": [
    "### 1.3 Loss function definition\n",
    "\n",
    "Usually cross-entropy (negative log-likelihood) is used as loss function for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellId": "3y7p7o6s7vecpf3kpktj8v",
    "id": "cGEhRWMYDMof"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def compute_loss(predictions, gt):\n",
    "    return F.cross_entropy(predictions, gt).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Optimizer and training schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our network using Adam with default parameters. \n",
    "\n",
    "For training by `torch.optim.SGD` you usually have to define training schedule - a way how to decrease learning rate during training. But since in adam all the gradients are scaled on their second momentum, the effect of a good training schedule is not so critical for training as in SGD. So we are going to act like lazy data scientists and will not decrease learning rate at all. But you may play with scheduling using for example `torch.optim.lr_scheduler.ExponentialLR`, see the [documentation](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) with explanation how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Training loop\n",
    "\n",
    "Let's combine the previously defined things together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellId": "w8rht9ygh7uns89ypozln",
    "id": "sEy0LiHxDMol",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def eval_model(model, data_generator):\n",
    "    accuracy = []\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_generator:\n",
    "            X_batch = X_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            y_pred = logits.max(1)[1].data\n",
    "            accuracy.append(np.mean((y_batch.cpu() == y_pred.cpu()).numpy()))\n",
    "    return np.mean(accuracy)\n",
    "\n",
    "            \n",
    "def train_model(model, optimizer, train_data_generator):\n",
    "    train_loss = []\n",
    "    model.train(True) # enable dropout / batch_norm training behavior\n",
    "    for (X_batch, y_batch) in tqdm.tqdm(train_data_generator):\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        # YOUR CODE: move X_batch, y_batch to 'device', compute model outputs on X_batch, \n",
    "        # run `compute_loss()` function\n",
    "#         print(torch.permute(X_batch, (0, 2, 3, 1)).shape)\n",
    "        X_batch = X_batch.to(device)\n",
    "        \n",
    "        y_batch = y_batch.to(device)\n",
    "        predictions = model(X_batch)\n",
    "        loss = compute_loss(predictions, y_batch)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # metrics\n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "\n",
    "def train_loop(model, optimizer, train_data_generator, val_data_generator, num_epochs):\n",
    "    \"\"\"\n",
    "    num_epochs - total amount of full passes over training data\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_model(model, optimizer, train_data_generator)\n",
    "        \n",
    "        val_accuracy = eval_model(model, val_data_generator)\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(train_loss))\n",
    "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Training\n",
    "\n",
    "All the preparation is done, time to run the training!\n",
    "\n",
    "Normally after training for 30 epochs you should get a neural network that predicts labels with >40% accuracy here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_loop(model, opt, train_batch_gen, val_batch_gen, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_accuracy = eval_model(model, val_batch_gen)\n",
    "# print(f\"Trained model accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40.21 - val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Say Hello to ResNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you need to redefine your model, all the rest will be the same. As with VGG, we are going to define ResNet-like model, not a classic architecture, designed for ImageNet classification.\n",
    "\n",
    "\"ResNet-like\" usually means that your network consists of \"residual blocks\". There are two types of blocks that widely used: with two convolutions and with three convolutions:\n",
    "![resnet_blocks](https://miro.medium.com/max/613/1*zS2ChIMwAqC5DQbL5yD9iQ.png)\n",
    "\n",
    "In practice, blocks with three convolutions are used often since they allows to build more deep network with less parameters. Blocks with two convolutions are usually used for comparisson with non-residual networks, espatially with VGG and AlexNet.\n",
    "\n",
    "Here is a table from the paper \"[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\" that describes classical configurations of ResNet networks. Usually they are referred as ResNet-18, ResNet-34 and so on using amount of layers as identificator. Note, that networks starting from ResNet-50 are based on 3-convolutional blocks. In fact ResNet-18 and ResNet-34 were introduces just for comparison with VGG, while ResNet-50 is what usually used in practice as a good baseline.\n",
    "\n",
    "![img](https://miro.medium.com/max/2400/1*aq0q7gCvuNUqnMHh4cpnIw.png)\n",
    "\n",
    "As with VGG, we are going to build our own config for network. Let's use 2-convolutional blocks for comparisson with vgg and take network like [Conv7x7 - 32] + [conv32-block, conv32-block] + [conv64-block, conv64-block] + [conv128-block, conv128-block] + [GlobalAveragePooling] + fc200 + softmax\n",
    "\n",
    "Comparing to ResNet18, we decreased the amount of filters and removed max-pooling in the beggining and the last set of convolutions for keeping meaningful spatial resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock2(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implements the following function:\n",
    "    \n",
    "    output = relu(F(input) + Residual(input)), where: \n",
    "        Residual(x) = Conv + bn + relu + conv + bn\n",
    "        F(x) = x                                        , if in_channels == out_channels and stride == 1\n",
    "             = Conv1x1(in_channel, out_channel, stride) , otherwise\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding='same'):\n",
    "        super().__init__()\n",
    "        # YOUR CODE: define conv1, bn1, relu1, conv2, bn2 for residual branch computation\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv3 = None  # conv for main branch adopatation\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.conv3 = nn.Conv2d(in_channels, out_channels, 1, stride, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE: compute residual branch, \n",
    "        # DON'T OVERRIDE 'x' as you will need it\n",
    "        \n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.conv3 is not None:\n",
    "            x = self.conv3(x)\n",
    "            \n",
    "        result = self.relu2(x + out)\n",
    "        return result\n",
    "\n",
    "def create_resnet_like_network():\n",
    "    model = nn.Sequential()\n",
    "    \n",
    "    config = [[32, 32], [64, 64], [128, 128]]\n",
    "    model.add_module('init_conv', ConvBNRelu(3, 32, kernel_size=7, stride=2, padding=3))\n",
    "    \n",
    "    in_channels = 32\n",
    "    for i in range(len(config)):\n",
    "        for j in range(len(config[i])):\n",
    "            out_channels = config[i][j]\n",
    "            stride = 2 if i != 0 and j == 0 else 1\n",
    "            # YOUR CODE: add ResNetBlock2 module to model\n",
    "            \n",
    "            resnet_block = ResNetBlock2(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                padding=1\n",
    "            )\n",
    "            model.add_module(f'resnet_block_{i}_{j}', resnet_block)\n",
    "            \n",
    "            in_channels = out_channels\n",
    "    model.add_module('pool', GlobalAveragePool((2,3)))\n",
    "    model.add_module('logits', nn.Linear(out_channels, 200))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our network then. Normally after training for 30 epochs you should get a neural network that predicts labels with >40% accuracy and gives near +1% profit to vgg-like network from the previous experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE: create resnet model, move it to 'device', create same optimizer as in previous experiment\n",
    "\n",
    "model_resnet = create_resnet_like_network()\n",
    "model_resnet = model_resnet.to(device)\n",
    "opt = torch.optim.Adam(model_resnet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loop(model_resnet, opt, train_batch_gen, val_batch_gen, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model accuracy: 0.42207404458598724\n"
     ]
    }
   ],
   "source": [
    "model_accuracy = eval_model(model_resnet, val_batch_gen)\n",
    "print(f\"Trained model accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 42.21 - val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were attentive to our resnet network, you may notice that it has almost 2x more parameters and 2x deeper than vgg-like network. Let's define comparable vgg-like network by doubling amount of conv layers.\n",
    "\n",
    "Our new vgg-like architecture will be [Conv(16), Conv(16), MaxPool] + [Conv(32), Conv(32), Conv(32), Conv(32), MaxPool] + [Conv(64), Conv(64), Conv(64), Conv(64), MaxPool] + [Conv(128), Conv(128), Conv(128), Conv(128)] + [GlobalAveragePooling] + [FC(200) + softmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_vgg = create_vgg_like_network(config=[[16,16], [32,32,32,32], [64, 64, 64, 64], [128, 128, 128, 128]])\n",
    "model_vgg = model_vgg.to(device)\n",
    "opt = torch.optim.Adam(model_vgg.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_loop(model_vgg, opt, train_batch_gen, val_batch_gen, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model accuracy: 0.38425557324840764\n"
     ]
    }
   ],
   "source": [
    "# model_accuracy = eval_model(model_vgg, val_batch_gen)\n",
    "# print(f\"Trained model accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 38.43 - val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the profit from residual connections? \n",
    "\n",
    "The quality of vgg network in this experiment could be even worse than the quality of vgg network in the first experiment. This is due to gradient vanishing problem that makes it hard to train deep neural networks without residual conections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Test time augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-time augmentation (TTA) is a powerful techneque that allows you to trade inference time for quality. The main idea is as follows. As for train data augmentation, you may use some image transformations to generate new representations of the input image and expect that on these representations properly trained network provides consistent predictions. These predictions can be averaged then in order to get more stable prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model accuracy: 0.42207404458598724\n"
     ]
    }
   ],
   "source": [
    "model_accuracy = eval_model(model_resnet, val_batch_gen)\n",
    "print(f\"Trained model accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_with_tta(model, data_generator, transformations, n_transformations):\n",
    "    accuracy = []\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm.tqdm(data_generator):\n",
    "            logits_per_transform = []\n",
    "            for _ in range(n_transformations):\n",
    "                # YOUR CODE: apply transformations to X_batch, move batch to device, run forward pass\n",
    "                # DON\"T OVERRIDE X_batch \n",
    "                #X_batch_transformed = ...\n",
    "                #logits = ...\n",
    "                X_batch_transformed = transformations(X_batch)\n",
    "                X_batch_transformed = X_batch_transformed.to(device)\n",
    "                \n",
    "                logits = model(X_batch_transformed)\n",
    "                \n",
    "                logits_per_transform.append(logits)\n",
    "                \n",
    "            # YOUR CODE: stack logits_per_transform and calculate mean over stacked dimension\n",
    "            #averaged_logits = ...\n",
    "            averaged_logits = torch.stack([logits for logits in logits_per_transform]).mean(dim=0)\n",
    "\n",
    "            y_pred = averaged_logits.max(dim=1)[1].data\n",
    "            accuracy.append(np.mean((y_batch.cpu() == y_pred.cpu()).numpy()))\n",
    "    return np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10839/2193008815.py:5: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  resample=PIL.Image.BILINEAR\n",
      "/home/german.petrov/Desktop/dvg_solutions/venv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:1292: UserWarning: The parameter 'resample' is deprecated since 0.12 and will be removed 0.14. Please use 'interpolation' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tta_transformations = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(\n",
    "        degrees=10,\n",
    "        resample=PIL.Image.BILINEAR\n",
    "    ),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=(0.75, 1.25),\n",
    "        contrast=(0.75, 1.25),\n",
    "        saturation=(0.75, 1.25),\n",
    "        hue=(-0.1, 0.1),\n",
    "    ),\n",
    "#     transforms.ToTensor(),\n",
    "    # YOUR CODE: add ColorJitter augmentation; probably it's good idea to reduce the parameters of \n",
    "    # jittering comparing to augmentation on train set in order to reduce variance\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:25<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy with 3 forward runs is 0.4606886942675159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:40<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy with 5 forward runs is 0.4658638535031847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:56<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy with 7 forward runs is 0.46865047770700635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [01:11<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy with 9 forward runs is 0.47113853503184716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [01:19<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy with 13 forward runs is 0.4703423566878981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [01:07<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy with 15 forward runs is 0.4720342356687898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_forwards = [1]\n",
    "tta_accuracy = [model_accuracy]\n",
    "for i in [3, 5, 7, 9, 13, 15]:\n",
    "    tta_accuracy.append(eval_model_with_tta(model_resnet, val_batch_gen, tta_transformations, n_transformations=i))\n",
    "    n_forwards.append(i)\n",
    "    print(f\"Model accuracy with {n_forwards[-1]} forward runs is {tta_accuracy[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what we have computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test time augmentation results')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(n_forwards, tta_accuracy)\n",
    "plt.grid()\n",
    "plt.xlabel('n_forwards per sample')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Test time augmentation results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally you should get 1-2% improvement of accuracy here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "Introducing of residual blocks played a big role in deep learning and allowed to train deep (and I mean really [DEEP](https://github.com/KaimingHe/resnet-1k-layers/blob/master/resnet-pre-act.lua#L2)) networks. Many modern architectures include such layer or its variation. For deeper understanding of influence of skip connections you can read the following papers:\n",
    "1. [\"Residual Networks Behave Like Ensembles of\n",
    "Relatively Shallow Networks\"](https://arxiv.org/pdf/1605.06431.pdf) - interesting point of view on residual blocks showing that statement \"skip connections solves vanishing gradients problem\" is ambigious in some way\n",
    "2. [\"Identity Mappings in Deep Residual Networks\"](https://arxiv.org/pdf/1603.05027.pdf) ([short summary](https://towardsdatascience.com/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e)) - study and comparison of different residual blocks variations showing that preserving \"identity path\" through the network improves quality\n",
    "3. [\"Visualizing the Loss Landscape of Neural Nets\"](https://arxiv.org/pdf/1712.09913.pdf) - some attempts on loss function visualization showed how skip-connections affect loss landscape"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "seminar_pytorch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "notebookId": "0bd81ca7-4175-4905-a84c-21ed8da72299"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
